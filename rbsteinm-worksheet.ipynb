{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers, spark_helpers\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4064\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project-rbsteinm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=project-rbsteinm>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe. We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')\n",
    "\n",
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))\n",
    "\n",
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402\n",
    "\n",
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]\n",
    "\n",
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)\n",
    "# create dict of stations from pandas dataframe\n",
    "stations = pandas_df.set_index('station_ID').to_dict('index')\n",
    "\n",
    "# dump metadata in pickle\n",
    "with open('./data/metadata.pickle', 'wb') as handle:\n",
    "    pickle.dump(stations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata from pickle\n",
    "with open('./data/metadata.pickle', 'rb') as handle:\n",
    "    stations = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load full data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', spark_helpers.get_weekday(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '2017-12-10 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discard ill-formated rows where the train leaves a station before arriving in it\n",
    "df2 = df2.where((df2.schedule_dep >= df2.schedule_arrival) | functions.col('schedule_arrival').isNull() | functions.col('schedule_dep').isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7207472"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7207450"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', spark_helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', spark_helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep', 'line']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', spark_helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type, trips2.schedule_dep,trips2.next_sched_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regenerate_models = False\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = (helpers.model_network(trips4, date))\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            helpers.network_to_datetime(network) # works inplace\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monday loaded\n",
      "tuesday loaded\n",
      "wednesday loaded\n",
      "thursday loaded\n",
      "friday loaded\n",
      "saturday loaded\n",
      "sunday loaded\n"
     ]
    }
   ],
   "source": [
    "# load the networks from the pickles\n",
    "models = []\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)\n",
    "    print(day + ' loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute walking network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking network loaded\n"
     ]
    }
   ],
   "source": [
    "# compute walking network\n",
    "walking_network = helpers.compute_walking_network(stations)\n",
    "print('walking network loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 916 from Küsnacht ZH, Bergstrasse to Zürich Tiefenbrunnen, Bahnhof 14:03 -> 14:16(13 stops)\n",
      "line 4 from Zürich Tiefenbrunnen, Bahnhof to Zürich, Opernhaus 14:17 -> 14:24(6 stops)\n",
      "line walk from Zürich, Opernhaus to Zürich Stadelhofen 14:24 -> 14:25(1 stops)\n",
      "line S6 from Zürich Stadelhofen to Zürich Hardbrücke 14:27 -> 14:33(2 stops)\n",
      "line S5 from Zürich Hardbrücke to Zürich Altstetten 14:41 -> 14:44(1 stops)\n",
      "line walk from Zürich Altstetten to Zürich Altstetten, Bahnhof 14:44 -> 14:44(1 stops)\n",
      "line 89 from Zürich Altstetten, Bahnhof to Zürich, Frankental 14:45 -> 14:51(5 stops)\n",
      "line walk from Zürich, Frankental to Oberengstringen, Eggbühl 14:51 -> 14:57(1 stops)\n",
      "line 308 from Oberengstringen, Eggbühl to Oberengstringen, Zentrum 15:02 -> 15:04(2 stops)\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "sp = helpers.shortest_path(models, walking_network, stations,8576218, 8590727, datetime(2018, 1, 15, 14))\n",
    "helpers.reduced_path_tostring(helpers.reduce_path(sp), stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % finished\n",
      "2.0 % finished\n",
      "4.0 % finished\n",
      "6.0 % finished\n",
      "8.0 % finished\n",
      "10.0 % finished\n",
      "12.0 % finished\n",
      "14.0 % finished\n",
      "16.0 % finished\n",
      "18.0 % finished\n",
      "20.0 % finished\n",
      "22.0 % finished\n",
      "24.0 % finished\n",
      "26.0 % finished\n",
      "28.0 % finished\n",
      "30.0 % finished\n",
      "32.0 % finished\n",
      "34.0 % finished\n",
      "36.0 % finished\n",
      "38.0 % finished\n",
      "40.0 % finished\n",
      "42.0 % finished\n",
      "44.0 % finished\n",
      "46.0 % finished\n",
      "48.0 % finished\n",
      "50.0 % finished\n",
      "52.0 % finished\n",
      "54.0 % finished\n",
      "56.0 % finished\n",
      "58.0 % finished\n",
      "60.0 % finished\n",
      "62.0 % finished\n",
      "64.0 % finished\n",
      "66.0 % finished\n",
      "68.0 % finished\n",
      "70.0 % finished\n",
      "72.0 % finished\n",
      "74.0 % finished\n",
      "76.0 % finished\n",
      "78.0 % finished\n",
      "80.0 % finished\n",
      "82.0 % finished\n",
      "84.0 % finished\n",
      "86.0 % finished\n",
      "88.0 % finished\n",
      "90.0 % finished\n",
      "92.0 % finished\n",
      "94.0 % finished\n",
      "96.0 % finished\n",
      "98.0 % finished\n"
     ]
    }
   ],
   "source": [
    "# select two stops at random and create a shortest path. Do that 100 times\n",
    "shortest_paths = []\n",
    "n_runs = 500\n",
    "date = datetime(2018, 1, 16, 14)\n",
    "source = 8503000 # zurich HB\n",
    "reachable_stations_ids = helpers.get_reachable_stations(models[date.weekday()], walking_network, source)\n",
    "reachable_stations = {sid: stations[sid] for sid in reachable_stations_ids}\n",
    "for i in range(n_runs):\n",
    "    if i%10==0:\n",
    "        print(100*i/n_runs, '% finished')\n",
    "    #source = random.choice(list(stations.keys()))\n",
    "    dest = random.choice(list(reachable_stations.keys()))\n",
    "    shortest_paths.append(helpers.shortest_path(models, walking_network, reachable_stations, source, dest, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path with arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_correspondance_reverse(edges, current_time, walking_network, source, dest, prev_tid):\n",
    "    \"\"\"\n",
    "    returns departure/arrival times of the first ride departing after the current time\n",
    "    assumes that the list current_time is sorted according to departure times!\n",
    "    prev_tid: trip_id of the edge that led to source. Allows us to check if there is a change of bus at dest,\n",
    "    in which case we will add one minute to the current time to take changing time into account\n",
    "    \n",
    "    Here since we are in reverse:\n",
    "    - we follow the reverse edge from u to v\n",
    "    - original edges goes v -> u, or dest -> source\n",
    "    - source==u and dest==v\n",
    "    - dep is the time at which you leave v and arr is the time at which you arrive at u\n",
    "    \"\"\"\n",
    "    tid, line = None,None\n",
    "    # if the edge exists for the rides (here source is v and dest is u)\n",
    "    if source in edges and dest in edges[source]:\n",
    "        times = edges[source][dest] # list of schedules form source to destinations\n",
    "        # index of the fist ride arriving before the current time\n",
    "        index = np.searchsorted([x[1] for x in times], current_time) - 1\n",
    "        # None if there is no ride that can make you arrive at this time\n",
    "        (dep,arr,tid,line) = times[index] if index >= 0 else (None,None,None,None)\n",
    "        # If you have less than one minute for a change of vehicule, that's not acceptable\n",
    "        # add 60 second to the current time and seach again for the next correspondance\n",
    "        if tid is not None and prev_tid!=tid and current_time==arr:\n",
    "            index = np.searchsorted([x[1] for x in times], current_time-timedelta(seconds=60)) - 1\n",
    "            (dep,arr,tid,line) = times[index] if index >= 0 else (None,None,None,None)\n",
    "    else:\n",
    "        (dep,arr) = (None,None) # None if there is no edge by vehicule\n",
    "        \n",
    "    # determine if you can reach dest from source by foot\n",
    "    if source in walking_network and dest in walking_network[source]:\n",
    "        walk_time = walking_network[source][dest]\n",
    "        (dep_walk,arr_walk) = (current_time-walk_time,current_time)\n",
    "    else:\n",
    "        (dep_walk,arr_walk) = (None,None)\n",
    "    \n",
    "    if dep is None and dep_walk is None:\n",
    "        return (None,None,None,None) # if you can neither walk nor take a transport\n",
    "    elif dep is None:\n",
    "        return (dep_walk,arr_walk,'walk','walk') # if you can only walk\n",
    "    elif dep_walk is None:\n",
    "        return (dep,arr,tid,line) # if you can only take a transport\n",
    "    else:\n",
    "        return (dep,arr,tid,line) if (dep>=dep_walk) else (dep_walk,arr_walk,'walk','walk') # if you can walk or ride, do the fastest\n",
    "    \n",
    "def shortest_path_reverse(models, walking_network, stations, source, destination, arrival_time):\n",
    "    '''\n",
    "    Compute the shortest path between destination and source using Dijksta's algorithm\n",
    "    models: contains 7 networks, one for each day of the week. They can be generated using model_network()\n",
    "    source, destination: station IDs\n",
    "    '''\n",
    "    edges = build_reverse_network(models[arrival_time.weekday()]) # get the network for the correct day of the week\n",
    "    Q = set(stations.keys()) # deep copy\n",
    "    dist = dict.fromkeys(Q, datetime.min) # distances to the source (= departure time at each node)\n",
    "    prev = dict.fromkeys(Q, (None, None, None, None, None)) # (previous node, dep/arr times, trip_id, line) in the shortest path\n",
    "    dist[destination] = arrival_time\n",
    "    \n",
    "    while Q:\n",
    "        unvisited_dist = {key: dist[key] for key in Q} # distances of unvisited nodes\n",
    "        u = max(unvisited_dist, key=unvisited_dist.get) # u <- vertex in Q with maximum dist[u]\n",
    "        #print('current node ',u)\n",
    "        \n",
    "        if dist[u] == datetime.min:\n",
    "            raise Exception('Only nodes with infinity distance in the queue. The graph is disconected')\n",
    "        \n",
    "        Q.remove(u) #remove u from Q\n",
    "        \n",
    "        # if this is the source node, we can terminate\n",
    "        if u == source:\n",
    "            path = []\n",
    "            # reconstruct the shortest path\n",
    "            while prev[u][0] != destination:\n",
    "                assert(prev[u][0] is not None),'no path from ' + stations[source].name + ' to ' + stations[destination].name\n",
    "                assert(len(path) < 300), 'Path has more than 300 hops, something is wrong ...'\n",
    "                current_edge = (prev[u][0],u,prev[u][1],prev[u][2],prev[u][3],prev[u][4])\n",
    "                #path.insert(0,current_edge)\n",
    "                path.append(current_edge)\n",
    "                u = prev[u][0] # get previous node\n",
    "            current_edge = (prev[u][0],u,prev[u][1],prev[u][2],prev[u][3],prev[u][4])\n",
    "            #path.insert(0,current_edge) # push the source at the beginning of the path\n",
    "            path.append(current_edge)\n",
    "            return path\n",
    "        \n",
    "        current_time = dist[u]\n",
    "        neighbors = set(edges[u].keys()) if u in edges else set() # u's neighbors by vehicule\n",
    "        walk_neighbors = set(walking_network[u].keys())\n",
    "        for v in neighbors.union(walk_neighbors):\n",
    "            # take the first correspondance. dep is the departure time from v and arr the arrival time at u\n",
    "            (dep_time,arr_time,tid,line) = get_next_correspondance_reverse(edges, current_time, walking_network, u, v, prev[u][3])\n",
    "            # there is no more correspondance for this edge\n",
    "            if dep_time is None:\n",
    "                continue\n",
    "            dist_u_v = arr_time - dep_time # travelling time from v to u\n",
    "            waiting_time = current_time -  arr_time # waiting time after your arrival at u\n",
    "            dep_v = current_time - waiting_time - dist_u_v # determine at what time you left v for u\n",
    "            # a shorter path to v has been found\n",
    "            if dep_v > dist[v]:\n",
    "                dist[v] = dep_v\n",
    "                prev[v] = (u,dep_time,arr_time,tid,line)\n",
    "            \n",
    "    raise Exception('No path was found from source to destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = shortest_path_reverse(models, walking_network, stations,8576218, 8590727, datetime(2018, 1, 15, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8576217,\n",
       "  8576218,\n",
       "  datetime.datetime(2018, 1, 15, 12, 29, 2, 47025),\n",
       "  datetime.datetime(2018, 1, 15, 12, 34),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8576216,\n",
       "  8576217,\n",
       "  datetime.datetime(2018, 1, 15, 12, 34),\n",
       "  datetime.datetime(2018, 1, 15, 12, 35),\n",
       "  '85:849:104820-01912-1',\n",
       "  '916'),\n",
       " (8576209,\n",
       "  8576216,\n",
       "  datetime.datetime(2018, 1, 15, 12, 36, 55, 139052),\n",
       "  datetime.datetime(2018, 1, 15, 12, 41, 57, 350685),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8576208,\n",
       "  8576209,\n",
       "  datetime.datetime(2018, 1, 15, 12, 41, 57, 350685),\n",
       "  datetime.datetime(2018, 1, 15, 12, 46, 20, 895690),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8576207,\n",
       "  8576208,\n",
       "  datetime.datetime(2018, 1, 15, 12, 46, 20, 895690),\n",
       "  datetime.datetime(2018, 1, 15, 12, 50, 34, 734539),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8576206,\n",
       "  8576207,\n",
       "  datetime.datetime(2018, 1, 15, 12, 50, 34, 734539),\n",
       "  datetime.datetime(2018, 1, 15, 12, 54),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8576205,\n",
       "  8576206,\n",
       "  datetime.datetime(2018, 1, 15, 12, 54),\n",
       "  datetime.datetime(2018, 1, 15, 12, 55),\n",
       "  '85:849:104749-01912-1',\n",
       "  '912'),\n",
       " (8576204,\n",
       "  8576205,\n",
       "  datetime.datetime(2018, 1, 15, 12, 56, 9, 218001),\n",
       "  datetime.datetime(2018, 1, 15, 12, 59, 3, 229680),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8503100,\n",
       "  8576204,\n",
       "  datetime.datetime(2018, 1, 15, 12, 59, 3, 229680),\n",
       "  datetime.datetime(2018, 1, 15, 13, 4),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8503004,\n",
       "  8503100,\n",
       "  datetime.datetime(2018, 1, 15, 13, 4),\n",
       "  datetime.datetime(2018, 1, 15, 13, 7),\n",
       "  '85:11:19648:001',\n",
       "  'S16'),\n",
       " (8503003,\n",
       "  8503004,\n",
       "  datetime.datetime(2018, 1, 15, 13, 8),\n",
       "  datetime.datetime(2018, 1, 15, 13, 10),\n",
       "  '85:11:19648:001',\n",
       "  'S16'),\n",
       " (8503000,\n",
       "  8503003,\n",
       "  datetime.datetime(2018, 1, 15, 13, 18),\n",
       "  datetime.datetime(2018, 1, 15, 13, 20),\n",
       "  '85:11:19548:001',\n",
       "  'S15'),\n",
       " (8503020,\n",
       "  8503000,\n",
       "  datetime.datetime(2018, 1, 15, 13, 22),\n",
       "  datetime.datetime(2018, 1, 15, 13, 24),\n",
       "  '85:11:19548:001',\n",
       "  'S15'),\n",
       " (8503001,\n",
       "  8503020,\n",
       "  datetime.datetime(2018, 1, 15, 13, 31),\n",
       "  datetime.datetime(2018, 1, 15, 13, 35),\n",
       "  '85:11:18348:001',\n",
       "  'S3'),\n",
       " (8503509,\n",
       "  8503001,\n",
       "  datetime.datetime(2018, 1, 15, 13, 36),\n",
       "  datetime.datetime(2018, 1, 15, 13, 38),\n",
       "  '85:11:18348:001',\n",
       "  'S3'),\n",
       " (8590790,\n",
       "  8503509,\n",
       "  datetime.datetime(2018, 1, 15, 13, 40, 20, 640773),\n",
       "  datetime.datetime(2018, 1, 15, 13, 46),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8590802,\n",
       "  8590790,\n",
       "  datetime.datetime(2018, 1, 15, 13, 46),\n",
       "  datetime.datetime(2018, 1, 15, 13, 46),\n",
       "  '85:849:301723-32301-1',\n",
       "  '308'),\n",
       " (8593529,\n",
       "  8590802,\n",
       "  datetime.datetime(2018, 1, 15, 13, 47),\n",
       "  datetime.datetime(2018, 1, 15, 13, 47),\n",
       "  '85:849:301723-32301-1',\n",
       "  '308'),\n",
       " (8590832,\n",
       "  8593529,\n",
       "  datetime.datetime(2018, 1, 15, 13, 48, 1, 340514),\n",
       "  datetime.datetime(2018, 1, 15, 13, 50, 7, 282365),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8590833,\n",
       "  8590832,\n",
       "  datetime.datetime(2018, 1, 15, 13, 50, 7, 282365),\n",
       "  datetime.datetime(2018, 1, 15, 13, 54, 29, 226793),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8590727,\n",
       "  8590833,\n",
       "  datetime.datetime(2018, 1, 15, 13, 54, 29, 226793),\n",
       "  datetime.datetime(2018, 1, 15, 14, 0),\n",
       "  'walk',\n",
       "  'walk')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_reverse_network(network):\n",
    "    '''\n",
    "    reverts all the edges in the network\n",
    "    '''\n",
    "    reverse_net = dict()\n",
    "    for source in network.keys():\n",
    "        for dest in network[source].keys():\n",
    "            if not dest in reverse_net.keys():\n",
    "                reverse_net[dest] = dict()\n",
    "            reverse_net[dest][source] = network[source][dest]\n",
    "    return reverse_net\n",
    "reverse_net = build_reverse_network(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
