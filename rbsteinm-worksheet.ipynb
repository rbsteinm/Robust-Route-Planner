{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project-rbsteinm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=project-rbsteinm>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe. We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RELOAD_METADATA = True\n",
    "if RELOAD_METADATA:\n",
    "    # load metadata\n",
    "    raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')\n",
    "\n",
    "    # remove multiple spaces\n",
    "    metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "    # split into columns\n",
    "    metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "    for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "        metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "    # remove useless column\n",
    "    metadata = metadata.drop('_c0')\n",
    "    # trim name column to remove left/right blank\n",
    "    metadata = metadata.withColumn('name', functions.trim(metadata.name))\n",
    "\n",
    "    # coordinates of Zürich main train station\n",
    "    lat_zurich = 47.3782\n",
    "    long_zurich = 8.5402\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    pandas_df = metadata.toPandas()\n",
    "    \n",
    "    # keep only the stops that are located < 10km from Zurich HB\n",
    "    pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "    pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]\n",
    "\n",
    "    # recreate spark dataframe from pandas dataframe\n",
    "    metadata = spark.createDataFrame(pandas_df)\n",
    "    # create dict of stations from pandas dataframe\n",
    "    stations = pandas_df.set_index('station_ID').to_dict('index')\n",
    "\n",
    "    # dump metadata in pickle\n",
    "    with open('./data/metadata.pickle', 'wb') as handle:\n",
    "        pickle.dump(stations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata from pickle\n",
    "with open('./data/metadata.pickle', 'rb') as handle:\n",
    "    stations = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load full data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', helpers.get_weekday(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '2017-12-10 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discard ill-formated rows where the train leaves a station before arriving in it\n",
    "df2 = df2.where((df2.schedule_dep >= df2.schedule_arrival) | functions.col('schedule_arrival').isNull() | functions.col('schedule_dep').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep', 'line']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type, trips2.schedule_dep,trips2.next_sched_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regenerate_models = False\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = (helpers.model_network(trips4, date))\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            helpers.network_to_datetime(network) # works inplace\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monday loaded\n",
      "tuesday loaded\n",
      "wednesday loaded\n",
      "thursday loaded\n",
      "friday loaded\n",
      "saturday loaded\n",
      "sunday loaded\n"
     ]
    }
   ],
   "source": [
    "# load the networks from the pickles\n",
    "models = []\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)\n",
    "    print(day + ' loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute walking network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking network loaded\n"
     ]
    }
   ],
   "source": [
    "# compute walking network\n",
    "walking_network = helpers.compute_walking_network(stations)\n",
    "print('walking network loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8576218,\n",
       "  8576217,\n",
       "  datetime.datetime(2018, 1, 15, 14, 3),\n",
       "  datetime.datetime(2018, 1, 15, 14, 4),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576217,\n",
       "  8576216,\n",
       "  datetime.datetime(2018, 1, 15, 14, 4),\n",
       "  datetime.datetime(2018, 1, 15, 14, 5),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576216,\n",
       "  8576209,\n",
       "  datetime.datetime(2018, 1, 15, 14, 5),\n",
       "  datetime.datetime(2018, 1, 15, 14, 6),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576209,\n",
       "  8576208,\n",
       "  datetime.datetime(2018, 1, 15, 14, 6),\n",
       "  datetime.datetime(2018, 1, 15, 14, 7),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576208,\n",
       "  8576207,\n",
       "  datetime.datetime(2018, 1, 15, 14, 7),\n",
       "  datetime.datetime(2018, 1, 15, 14, 8),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576207,\n",
       "  8576206,\n",
       "  datetime.datetime(2018, 1, 15, 14, 9),\n",
       "  datetime.datetime(2018, 1, 15, 14, 9),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576206,\n",
       "  8576205,\n",
       "  datetime.datetime(2018, 1, 15, 14, 10),\n",
       "  datetime.datetime(2018, 1, 15, 14, 10),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576205,\n",
       "  8576204,\n",
       "  datetime.datetime(2018, 1, 15, 14, 10),\n",
       "  datetime.datetime(2018, 1, 15, 14, 11),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576204,\n",
       "  8576203,\n",
       "  datetime.datetime(2018, 1, 15, 14, 11),\n",
       "  datetime.datetime(2018, 1, 15, 14, 13),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576203,\n",
       "  8576202,\n",
       "  datetime.datetime(2018, 1, 15, 14, 13),\n",
       "  datetime.datetime(2018, 1, 15, 14, 14),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576202,\n",
       "  8576201,\n",
       "  datetime.datetime(2018, 1, 15, 14, 14),\n",
       "  datetime.datetime(2018, 1, 15, 14, 15),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576201,\n",
       "  8576182,\n",
       "  datetime.datetime(2018, 1, 15, 14, 15),\n",
       "  datetime.datetime(2018, 1, 15, 14, 16),\n",
       "  '85:849:104861-01912-1',\n",
       "  '916'),\n",
       " (8576182,\n",
       "  8576200,\n",
       "  datetime.datetime(2018, 1, 15, 14, 17),\n",
       "  datetime.datetime(2018, 1, 15, 14, 18),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576200,\n",
       "  8576199,\n",
       "  datetime.datetime(2018, 1, 15, 14, 18),\n",
       "  datetime.datetime(2018, 1, 15, 14, 19),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576199,\n",
       "  8576198,\n",
       "  datetime.datetime(2018, 1, 15, 14, 19),\n",
       "  datetime.datetime(2018, 1, 15, 14, 20),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576198,\n",
       "  8576197,\n",
       "  datetime.datetime(2018, 1, 15, 14, 20),\n",
       "  datetime.datetime(2018, 1, 15, 14, 21),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576197,\n",
       "  8576196,\n",
       "  datetime.datetime(2018, 1, 15, 14, 21),\n",
       "  datetime.datetime(2018, 1, 15, 14, 22),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576196,\n",
       "  8576195,\n",
       "  datetime.datetime(2018, 1, 15, 14, 23),\n",
       "  datetime.datetime(2018, 1, 15, 14, 24),\n",
       "  '85:3849:80930-02004-1',\n",
       "  '4'),\n",
       " (8576195,\n",
       "  8503003,\n",
       "  datetime.datetime(2018, 1, 15, 14, 24),\n",
       "  datetime.datetime(2018, 1, 15, 14, 25, 44, 784423),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8503003,\n",
       "  8503000,\n",
       "  datetime.datetime(2018, 1, 15, 14, 27),\n",
       "  datetime.datetime(2018, 1, 15, 14, 29),\n",
       "  '85:11:18654:001',\n",
       "  'S6'),\n",
       " (8503000,\n",
       "  8503020,\n",
       "  datetime.datetime(2018, 1, 15, 14, 29),\n",
       "  datetime.datetime(2018, 1, 15, 14, 31),\n",
       "  '85:11:18352:001',\n",
       "  'S3'),\n",
       " (8503020,\n",
       "  8503001,\n",
       "  datetime.datetime(2018, 1, 15, 14, 31),\n",
       "  datetime.datetime(2018, 1, 15, 14, 35),\n",
       "  '85:11:18352:001',\n",
       "  'S3'),\n",
       " (8503001,\n",
       "  8591057,\n",
       "  datetime.datetime(2018, 1, 15, 14, 35),\n",
       "  datetime.datetime(2018, 1, 15, 14, 36, 9, 650259),\n",
       "  'walk',\n",
       "  'walk'),\n",
       " (8591057,\n",
       "  8591434,\n",
       "  datetime.datetime(2018, 1, 15, 14, 39),\n",
       "  datetime.datetime(2018, 1, 15, 14, 41),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8591434,\n",
       "  8591402,\n",
       "  datetime.datetime(2018, 1, 15, 14, 41),\n",
       "  datetime.datetime(2018, 1, 15, 14, 41),\n",
       "  '85:849:91676-18089-1',\n",
       "  '89'),\n",
       " (8591402,\n",
       "  8591197,\n",
       "  datetime.datetime(2018, 1, 15, 14, 41),\n",
       "  datetime.datetime(2018, 1, 15, 14, 42),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8591197,\n",
       "  8591436,\n",
       "  datetime.datetime(2018, 1, 15, 14, 42),\n",
       "  datetime.datetime(2018, 1, 15, 14, 43),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8591436,\n",
       "  8591136,\n",
       "  datetime.datetime(2018, 1, 15, 14, 43),\n",
       "  datetime.datetime(2018, 1, 15, 14, 45),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8591136,\n",
       "  8590725,\n",
       "  datetime.datetime(2018, 1, 15, 14, 46),\n",
       "  datetime.datetime(2018, 1, 15, 14, 47),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8590725,\n",
       "  8590726,\n",
       "  datetime.datetime(2018, 1, 15, 14, 47),\n",
       "  datetime.datetime(2018, 1, 15, 14, 48),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8590726,\n",
       "  8590728,\n",
       "  datetime.datetime(2018, 1, 15, 14, 48),\n",
       "  datetime.datetime(2018, 1, 15, 14, 49),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304'),\n",
       " (8590728,\n",
       "  8590727,\n",
       "  datetime.datetime(2018, 1, 15, 14, 49),\n",
       "  datetime.datetime(2018, 1, 15, 14, 50),\n",
       "  '85:849:302080-32301-1',\n",
       "  '304')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "helpers.shortest_path(models, walking_network, stations,8576218, 8590727, datetime(2018, 1, 15, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,)+(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select two stops at random and create a shortest path. Do that 100 times\n",
    "import random\n",
    "shortest_paths = []\n",
    "for i in range(100):\n",
    "    if i %10==0:\n",
    "        print(i, '% finished')\n",
    "    source = random.choice(list(stations.keys()))\n",
    "    dest = random.choice(list(stations.keys()))\n",
    "    shortest_paths.append(shortest_path(models, walking_network, stations, source, dest, datetime(2018, 1, 15, 14)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "for path in shortest_paths:\n",
    "    dep = path[0][1]\n",
    "    arr = path[-1][2]\n",
    "    times.append(round((arr-dep).total_seconds()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhops = [len(path) for path in shortest_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(nhops)\n",
    "nhops.index(min(nhops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times.index(min(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortest_paths[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = (shortest_paths[23][0][0])\n",
    "d = (shortest_paths[23][-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
