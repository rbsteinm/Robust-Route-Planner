{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project-rbsteinm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=project-rbsteinm>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------+---------+------+\n",
      "|            name|station_ID|     long|      lat|height|\n",
      "+----------------+----------+---------+---------+------+\n",
      "|       Bucuresti|         2|26.074412| 44.44677|     0|\n",
      "|          Calais|         3| 1.811446|50.901549|     0|\n",
      "|      Canterbury|         4| 1.075329|51.284212|     0|\n",
      "|          Exeter|         5|-3.543547|50.729172|     0|\n",
      "|Fideris, Bahnhof|         7| 9.733756|46.922368|   744|\n",
      "+----------------+----------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25935"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas_df.distance_to_zh.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)\n",
    "# create dict of stations from pandas dataframe\n",
    "stations = pandas_df.set_index('station_ID').to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load full data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', helpers.get_weekday(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '10.12.2017 00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trips2.where(trips2.stop_type=='first').count()\n",
    "#trips2.where(trips2.stop_type=='last').count()\n",
    "#trips2.where(trips2.stop_type=='mid').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trips4.select('stop_id', 'next_sid').distinct().count()\n",
    "# gives 6606\n",
    "\n",
    "# trips3.select('stop_id', 'next_sid').distinct().count()\n",
    "# gives 8557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monday done\n",
      "tuesday done\n",
      "wednesday done\n",
      "thursday done\n",
      "friday done\n",
      "saturday done\n",
      "sunday done\n"
     ]
    }
   ],
   "source": [
    "regenerate_models = True\n",
    "models = []\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = helpers.model_network(trips4, date)\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the networks from the pickles\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8503001: [('2018-01-15 05:34:00', '2018-01-15 05:38:00'),\n",
       "  ('2018-01-15 05:49:00', '2018-01-15 05:53:00'),\n",
       "  ('2018-01-15 06:04:00', '2018-01-15 06:08:00'),\n",
       "  ('2018-01-15 06:19:00', '2018-01-15 06:23:00'),\n",
       "  ('2018-01-15 06:34:00', '2018-01-15 06:38:00'),\n",
       "  ('2018-01-15 06:49:00', '2018-01-15 06:53:00'),\n",
       "  ('2018-01-15 07:04:00', '2018-01-15 07:08:00'),\n",
       "  ('2018-01-15 07:19:00', '2018-01-15 07:23:00'),\n",
       "  ('2018-01-15 07:34:00', '2018-01-15 07:38:00'),\n",
       "  ('2018-01-15 07:49:00', '2018-01-15 07:53:00'),\n",
       "  ('2018-01-15 08:04:00', '2018-01-15 08:08:00'),\n",
       "  ('2018-01-15 08:19:00', '2018-01-15 08:23:00'),\n",
       "  ('2018-01-15 08:34:00', '2018-01-15 08:38:00'),\n",
       "  ('2018-01-15 08:49:00', '2018-01-15 08:53:00'),\n",
       "  ('2018-01-15 09:04:00', '2018-01-15 09:08:00'),\n",
       "  ('2018-01-15 09:19:00', '2018-01-15 09:23:00'),\n",
       "  ('2018-01-15 09:34:00', '2018-01-15 09:38:00'),\n",
       "  ('2018-01-15 09:49:00', '2018-01-15 09:53:00'),\n",
       "  ('2018-01-15 10:04:00', '2018-01-15 10:08:00'),\n",
       "  ('2018-01-15 10:19:00', '2018-01-15 10:23:00'),\n",
       "  ('2018-01-15 10:34:00', '2018-01-15 10:38:00'),\n",
       "  ('2018-01-15 10:49:00', '2018-01-15 10:53:00'),\n",
       "  ('2018-01-15 11:04:00', '2018-01-15 11:08:00'),\n",
       "  ('2018-01-15 11:19:00', '2018-01-15 11:23:00'),\n",
       "  ('2018-01-15 11:34:00', '2018-01-15 11:38:00'),\n",
       "  ('2018-01-15 11:49:00', '2018-01-15 11:53:00'),\n",
       "  ('2018-01-15 12:04:00', '2018-01-15 12:08:00'),\n",
       "  ('2018-01-15 12:19:00', '2018-01-15 12:23:00'),\n",
       "  ('2018-01-15 12:34:00', '2018-01-15 12:38:00'),\n",
       "  ('2018-01-15 12:49:00', '2018-01-15 12:53:00'),\n",
       "  ('2018-01-15 13:04:00', '2018-01-15 13:08:00'),\n",
       "  ('2018-01-15 13:19:00', '2018-01-15 13:23:00'),\n",
       "  ('2018-01-15 13:34:00', '2018-01-15 13:38:00'),\n",
       "  ('2018-01-15 13:49:00', '2018-01-15 13:53:00'),\n",
       "  ('2018-01-15 14:04:00', '2018-01-15 14:08:00'),\n",
       "  ('2018-01-15 14:19:00', '2018-01-15 14:23:00'),\n",
       "  ('2018-01-15 14:34:00', '2018-01-15 14:38:00'),\n",
       "  ('2018-01-15 14:49:00', '2018-01-15 14:53:00'),\n",
       "  ('2018-01-15 15:04:00', '2018-01-15 15:08:00'),\n",
       "  ('2018-01-15 15:19:00', '2018-01-15 15:23:00'),\n",
       "  ('2018-01-15 15:34:00', '2018-01-15 15:38:00'),\n",
       "  ('2018-01-15 15:49:00', '2018-01-15 15:53:00'),\n",
       "  ('2018-01-15 16:04:00', '2018-01-15 16:08:00'),\n",
       "  ('2018-01-15 16:19:00', '2018-01-15 16:23:00'),\n",
       "  ('2018-01-15 16:34:00', '2018-01-15 16:38:00'),\n",
       "  ('2018-01-15 16:49:00', '2018-01-15 16:53:00'),\n",
       "  ('2018-01-15 17:04:00', '2018-01-15 17:08:00'),\n",
       "  ('2018-01-15 17:19:00', '2018-01-15 17:23:00'),\n",
       "  ('2018-01-15 17:34:00', '2018-01-15 17:38:00'),\n",
       "  ('2018-01-15 17:49:00', '2018-01-15 17:53:00'),\n",
       "  ('2018-01-15 18:04:00', '2018-01-15 18:08:00'),\n",
       "  ('2018-01-15 18:19:00', '2018-01-15 18:23:00'),\n",
       "  ('2018-01-15 18:34:00', '2018-01-15 18:38:00'),\n",
       "  ('2018-01-15 18:49:00', '2018-01-15 18:53:00'),\n",
       "  ('2018-01-15 19:04:00', '2018-01-15 19:08:00'),\n",
       "  ('2018-01-15 19:19:00', '2018-01-15 19:23:00'),\n",
       "  ('2018-01-15 19:34:00', '2018-01-15 19:38:00'),\n",
       "  ('2018-01-15 19:49:00', '2018-01-15 19:53:00'),\n",
       "  ('2018-01-15 20:04:00', '2018-01-15 20:08:00'),\n",
       "  ('2018-01-15 20:19:00', '2018-01-15 20:23:00'),\n",
       "  ('2018-01-15 20:34:00', '2018-01-15 20:38:00'),\n",
       "  ('2018-01-15 20:49:00', '2018-01-15 20:53:00'),\n",
       "  ('2018-01-15 21:04:00', '2018-01-15 21:08:00'),\n",
       "  ('2018-01-15 21:19:00', '2018-01-15 21:23:00'),\n",
       "  ('2018-01-15 21:34:00', '2018-01-15 21:38:00'),\n",
       "  ('2018-01-15 21:49:00', '2018-01-15 21:53:00'),\n",
       "  ('2018-01-15 22:04:00', '2018-01-15 22:08:00'),\n",
       "  ('2018-01-15 22:19:00', '2018-01-15 22:23:00'),\n",
       "  ('2018-01-15 22:34:00', '2018-01-15 22:38:00'),\n",
       "  ('2018-01-15 22:49:00', '2018-01-15 22:53:00'),\n",
       "  ('2018-01-15 23:04:00', '2018-01-15 23:08:00'),\n",
       "  ('2018-01-15 23:19:00', '2018-01-15 23:23:00'),\n",
       "  ('2018-01-15 23:34:00', '2018-01-15 23:38:00'),\n",
       "  ('2018-01-15 23:49:00', '2018-01-15 23:53:00'),\n",
       "  ('2018-01-16 00:04:00', '2018-01-16 00:08:00'),\n",
       "  ('2018-01-16 00:31:00', '2018-01-16 00:36:00')],\n",
       " 8503512: [('2018-01-15 04:53:00', '2018-01-15 04:55:00'),\n",
       "  ('2018-01-15 05:38:00', '2018-01-15 05:40:00'),\n",
       "  ('2018-01-15 05:53:00', '2018-01-15 05:55:00'),\n",
       "  ('2018-01-15 06:08:00', '2018-01-15 06:10:00'),\n",
       "  ('2018-01-15 06:23:00', '2018-01-15 06:25:00'),\n",
       "  ('2018-01-15 06:38:00', '2018-01-15 06:40:00'),\n",
       "  ('2018-01-15 06:53:00', '2018-01-15 06:55:00'),\n",
       "  ('2018-01-15 07:08:00', '2018-01-15 07:10:00'),\n",
       "  ('2018-01-15 07:23:00', '2018-01-15 07:25:00'),\n",
       "  ('2018-01-15 07:38:00', '2018-01-15 07:40:00'),\n",
       "  ('2018-01-15 07:53:00', '2018-01-15 07:55:00'),\n",
       "  ('2018-01-15 08:08:00', '2018-01-15 08:10:00'),\n",
       "  ('2018-01-15 08:23:00', '2018-01-15 08:25:00'),\n",
       "  ('2018-01-15 08:38:00', '2018-01-15 08:40:00'),\n",
       "  ('2018-01-15 08:53:00', '2018-01-15 08:55:00'),\n",
       "  ('2018-01-15 09:08:00', '2018-01-15 09:10:00'),\n",
       "  ('2018-01-15 09:23:00', '2018-01-15 09:25:00'),\n",
       "  ('2018-01-15 09:38:00', '2018-01-15 09:40:00'),\n",
       "  ('2018-01-15 09:53:00', '2018-01-15 09:55:00'),\n",
       "  ('2018-01-15 10:08:00', '2018-01-15 10:10:00'),\n",
       "  ('2018-01-15 10:23:00', '2018-01-15 10:25:00'),\n",
       "  ('2018-01-15 10:38:00', '2018-01-15 10:40:00'),\n",
       "  ('2018-01-15 10:53:00', '2018-01-15 10:55:00'),\n",
       "  ('2018-01-15 11:08:00', '2018-01-15 11:10:00'),\n",
       "  ('2018-01-15 11:23:00', '2018-01-15 11:25:00'),\n",
       "  ('2018-01-15 11:38:00', '2018-01-15 11:40:00'),\n",
       "  ('2018-01-15 11:53:00', '2018-01-15 11:55:00'),\n",
       "  ('2018-01-15 12:08:00', '2018-01-15 12:10:00'),\n",
       "  ('2018-01-15 12:23:00', '2018-01-15 12:25:00'),\n",
       "  ('2018-01-15 12:38:00', '2018-01-15 12:40:00'),\n",
       "  ('2018-01-15 12:53:00', '2018-01-15 12:55:00'),\n",
       "  ('2018-01-15 13:08:00', '2018-01-15 13:10:00'),\n",
       "  ('2018-01-15 13:23:00', '2018-01-15 13:25:00'),\n",
       "  ('2018-01-15 13:38:00', '2018-01-15 13:40:00'),\n",
       "  ('2018-01-15 13:53:00', '2018-01-15 13:55:00'),\n",
       "  ('2018-01-15 14:08:00', '2018-01-15 14:10:00'),\n",
       "  ('2018-01-15 14:23:00', '2018-01-15 14:25:00'),\n",
       "  ('2018-01-15 14:38:00', '2018-01-15 14:40:00'),\n",
       "  ('2018-01-15 14:53:00', '2018-01-15 14:55:00'),\n",
       "  ('2018-01-15 15:08:00', '2018-01-15 15:10:00'),\n",
       "  ('2018-01-15 15:23:00', '2018-01-15 15:25:00'),\n",
       "  ('2018-01-15 15:38:00', '2018-01-15 15:40:00'),\n",
       "  ('2018-01-15 15:53:00', '2018-01-15 15:55:00'),\n",
       "  ('2018-01-15 16:08:00', '2018-01-15 16:10:00'),\n",
       "  ('2018-01-15 16:23:00', '2018-01-15 16:25:00'),\n",
       "  ('2018-01-15 16:38:00', '2018-01-15 16:40:00'),\n",
       "  ('2018-01-15 16:53:00', '2018-01-15 16:55:00'),\n",
       "  ('2018-01-15 17:08:00', '2018-01-15 17:10:00'),\n",
       "  ('2018-01-15 17:23:00', '2018-01-15 17:25:00'),\n",
       "  ('2018-01-15 17:38:00', '2018-01-15 17:40:00'),\n",
       "  ('2018-01-15 17:53:00', '2018-01-15 17:55:00'),\n",
       "  ('2018-01-15 18:08:00', '2018-01-15 18:10:00'),\n",
       "  ('2018-01-15 18:23:00', '2018-01-15 18:25:00'),\n",
       "  ('2018-01-15 18:38:00', '2018-01-15 18:40:00'),\n",
       "  ('2018-01-15 18:53:00', '2018-01-15 18:55:00'),\n",
       "  ('2018-01-15 19:08:00', '2018-01-15 19:10:00'),\n",
       "  ('2018-01-15 19:23:00', '2018-01-15 19:25:00'),\n",
       "  ('2018-01-15 19:38:00', '2018-01-15 19:40:00'),\n",
       "  ('2018-01-15 19:53:00', '2018-01-15 19:55:00'),\n",
       "  ('2018-01-15 20:08:00', '2018-01-15 20:10:00'),\n",
       "  ('2018-01-15 20:23:00', '2018-01-15 20:25:00'),\n",
       "  ('2018-01-15 20:38:00', '2018-01-15 20:40:00'),\n",
       "  ('2018-01-15 20:53:00', '2018-01-15 20:55:00'),\n",
       "  ('2018-01-15 21:08:00', '2018-01-15 21:10:00'),\n",
       "  ('2018-01-15 21:23:00', '2018-01-15 21:25:00'),\n",
       "  ('2018-01-15 21:38:00', '2018-01-15 21:40:00'),\n",
       "  ('2018-01-15 21:53:00', '2018-01-15 21:55:00'),\n",
       "  ('2018-01-15 22:08:00', '2018-01-15 22:10:00'),\n",
       "  ('2018-01-15 22:23:00', '2018-01-15 22:25:00'),\n",
       "  ('2018-01-15 22:38:00', '2018-01-15 22:40:00'),\n",
       "  ('2018-01-15 22:53:00', '2018-01-15 22:55:00'),\n",
       "  ('2018-01-15 23:08:00', '2018-01-15 23:10:00'),\n",
       "  ('2018-01-15 23:23:00', '2018-01-15 23:25:00'),\n",
       "  ('2018-01-15 23:38:00', '2018-01-15 23:40:00'),\n",
       "  ('2018-01-15 23:53:00', '2018-01-15 23:55:00'),\n",
       "  ('2018-01-16 00:08:00', '2018-01-16 00:10:00'),\n",
       "  ('2018-01-16 00:23:00', '2018-01-16 00:25:00')]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0][list(models[0].keys())[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
