{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project-fouche</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=project-fouche>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------+---------+------+\n",
      "|            name|station_ID|     long|      lat|height|\n",
      "+----------------+----------+---------+---------+------+\n",
      "|       Bucuresti|         2|26.074412| 44.44677|     0|\n",
      "|          Calais|         3| 1.811446|50.901549|     0|\n",
      "|      Canterbury|         4| 1.075329|51.284212|     0|\n",
      "|          Exeter|         5|-3.543547|50.729172|     0|\n",
      "|Fideris, Bahnhof|         7| 9.733756|46.922368|   744|\n",
      "+----------------+----------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25935"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_df.distance_to_zh.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)\n",
    "# create dict of stations from pandas dataframe\n",
    "stations = pandas_df.set_index('station_ID').to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', helpers.get_weekday(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '10.12.2017 00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep', 'transport_type', 'train_type', 'arr_forecast_status', 'weekday', 'real_arrival']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"arr_forecast_status\").over(w).alias(\"next_arr_forecast_status\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"real_arrival\").over(w).alias(\"next_real_arrival\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trips2.where(trips2.stop_type=='first').count()\n",
    "#trips2.where(trips2.stop_type=='last').count()\n",
    "#trips2.where(trips2.stop_type=='mid').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips4.select('stop_id', 'next_sid').distinct().count()\n",
    "# gives 6606\n",
    "\n",
    "# trips3.select('stop_id', 'next_sid').distinct().count()\n",
    "# gives 8557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regenerate_models = False\n",
    "models = []\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = helpers.model_network(trips4, date)\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the networks from the pickles\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0][list(models[0].keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction / Regression\n",
    "\n",
    "Deux idées : Le but c'est de prévoir le retard ou l'avance d'un départ ou d'une arrivée pour un arrêt précis à un moment précis et un trip précis.\n",
    "\n",
    "- Faire une vraie regression ou n'importe quel algo pour déterminer le retard. On utilise comme features : l'arrêt (catégorie), latitude longitude (peut être mettre ensemble en tuple), jour precis ou jour de la semaine ?, le type de transport (bus-train ...), le trip id ou redondant ???, \n",
    "\n",
    "- Faire une moyenne, à un arrêt, un trip, un jour, une heure la moyenne de retard qu'il a eu dans ses conditions ... \n",
    "\n",
    "On va avoir besoin des trips id dans le dico histoire de pouvoir lier le graph des arrêts avec le réél trajet, quel bus/train fait cet edge.\n",
    "\n",
    "### Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips5 = trips4.drop('stop_type', 'next_type', 'is_valid', 'arr_forcast_status', \n",
    "                     'schedule_time', 'date', 'schedule_arrival', 'next_sched_dep', \n",
    "                     'next_time', 'next_tid', 'real_arrival', 'arr_forecast_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>schedule_dep</th>\n",
       "      <th>transport_type</th>\n",
       "      <th>train_type</th>\n",
       "      <th>arr_forecast_status</th>\n",
       "      <th>weekday</th>\n",
       "      <th>next_sid</th>\n",
       "      <th>next_sched_arr</th>\n",
       "      <th>next_arr_forecast_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-28 02:12:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502222</td>\n",
       "      <td>2018-01-28 02:16:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-28 02:07:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-28 02:12:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-28 02:06:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-28 02:07:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-28 02:01:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-28 02:06:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-28 01:59:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-28 02:01:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503000</td>\n",
       "      <td>2018-01-28 01:57:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-28 01:59:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503003</td>\n",
       "      <td>2018-01-28 01:52:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8503000</td>\n",
       "      <td>2018-01-28 01:55:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-27 02:12:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8502222</td>\n",
       "      <td>2018-01-27 02:16:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-27 02:07:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-27 02:12:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-27 02:06:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-27 02:07:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-27 02:01:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-27 02:06:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-27 01:59:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-27 02:01:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503000</td>\n",
       "      <td>2018-01-27 01:57:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-27 01:59:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503003</td>\n",
       "      <td>2018-01-27 01:52:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>5</td>\n",
       "      <td>8503000</td>\n",
       "      <td>2018-01-27 01:55:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-21 02:12:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502222</td>\n",
       "      <td>2018-01-21 02:16:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-21 02:07:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502221</td>\n",
       "      <td>2018-01-21 02:12:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-21 02:06:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502229</td>\n",
       "      <td>2018-01-21 02:07:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-21 02:01:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8502220</td>\n",
       "      <td>2018-01-21 02:06:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-21 01:59:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8503001</td>\n",
       "      <td>2018-01-21 02:01:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85:11:13752:001</td>\n",
       "      <td>8503000</td>\n",
       "      <td>2018-01-21 01:57:00</td>\n",
       "      <td>Zug</td>\n",
       "      <td>SN</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "      <td>6</td>\n",
       "      <td>8503020</td>\n",
       "      <td>2018-01-21 01:59:00</td>\n",
       "      <td>GESCHAETZT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            trip_id  stop_id         schedule_dep transport_type train_type  \\\n",
       "0   85:11:13752:001  8502221  2018-01-28 02:12:00            Zug         SN   \n",
       "1   85:11:13752:001  8502229  2018-01-28 02:07:00            Zug         SN   \n",
       "2   85:11:13752:001  8502220  2018-01-28 02:06:00            Zug         SN   \n",
       "3   85:11:13752:001  8503001  2018-01-28 02:01:00            Zug         SN   \n",
       "4   85:11:13752:001  8503020  2018-01-28 01:59:00            Zug         SN   \n",
       "5   85:11:13752:001  8503000  2018-01-28 01:57:00            Zug         SN   \n",
       "6   85:11:13752:001  8503003  2018-01-28 01:52:00            Zug         SN   \n",
       "7   85:11:13752:001  8502221  2018-01-27 02:12:00            Zug         SN   \n",
       "8   85:11:13752:001  8502229  2018-01-27 02:07:00            Zug         SN   \n",
       "9   85:11:13752:001  8502220  2018-01-27 02:06:00            Zug         SN   \n",
       "10  85:11:13752:001  8503001  2018-01-27 02:01:00            Zug         SN   \n",
       "11  85:11:13752:001  8503020  2018-01-27 01:59:00            Zug         SN   \n",
       "12  85:11:13752:001  8503000  2018-01-27 01:57:00            Zug         SN   \n",
       "13  85:11:13752:001  8503003  2018-01-27 01:52:00            Zug         SN   \n",
       "14  85:11:13752:001  8502221  2018-01-21 02:12:00            Zug         SN   \n",
       "15  85:11:13752:001  8502229  2018-01-21 02:07:00            Zug         SN   \n",
       "16  85:11:13752:001  8502220  2018-01-21 02:06:00            Zug         SN   \n",
       "17  85:11:13752:001  8503001  2018-01-21 02:01:00            Zug         SN   \n",
       "18  85:11:13752:001  8503020  2018-01-21 01:59:00            Zug         SN   \n",
       "19  85:11:13752:001  8503000  2018-01-21 01:57:00            Zug         SN   \n",
       "\n",
       "   arr_forecast_status weekday  next_sid       next_sched_arr  \\\n",
       "0           GESCHAETZT       6   8502222  2018-01-28 02:16:00   \n",
       "1           GESCHAETZT       6   8502221  2018-01-28 02:12:00   \n",
       "2           GESCHAETZT       6   8502229  2018-01-28 02:07:00   \n",
       "3           GESCHAETZT       6   8502220  2018-01-28 02:06:00   \n",
       "4           GESCHAETZT       6   8503001  2018-01-28 02:01:00   \n",
       "5           GESCHAETZT       6   8503020  2018-01-28 01:59:00   \n",
       "6           GESCHAETZT       6   8503000  2018-01-28 01:55:00   \n",
       "7           GESCHAETZT       5   8502222  2018-01-27 02:16:00   \n",
       "8           GESCHAETZT       5   8502221  2018-01-27 02:12:00   \n",
       "9           GESCHAETZT       5   8502229  2018-01-27 02:07:00   \n",
       "10          GESCHAETZT       5   8502220  2018-01-27 02:06:00   \n",
       "11          GESCHAETZT       5   8503001  2018-01-27 02:01:00   \n",
       "12          GESCHAETZT       5   8503020  2018-01-27 01:59:00   \n",
       "13          GESCHAETZT       5   8503000  2018-01-27 01:55:00   \n",
       "14          GESCHAETZT       6   8502222  2018-01-21 02:16:00   \n",
       "15          GESCHAETZT       6   8502221  2018-01-21 02:12:00   \n",
       "16          GESCHAETZT       6   8502229  2018-01-21 02:07:00   \n",
       "17          GESCHAETZT       6   8502220  2018-01-21 02:06:00   \n",
       "18          GESCHAETZT       6   8503001  2018-01-21 02:01:00   \n",
       "19          GESCHAETZT       6   8503020  2018-01-21 01:59:00   \n",
       "\n",
       "   next_arr_forecast_status  \n",
       "0                GESCHAETZT  \n",
       "1                GESCHAETZT  \n",
       "2                GESCHAETZT  \n",
       "3                GESCHAETZT  \n",
       "4                GESCHAETZT  \n",
       "5                GESCHAETZT  \n",
       "6                GESCHAETZT  \n",
       "7                GESCHAETZT  \n",
       "8                GESCHAETZT  \n",
       "9                GESCHAETZT  \n",
       "10               GESCHAETZT  \n",
       "11               GESCHAETZT  \n",
       "12               GESCHAETZT  \n",
       "13               GESCHAETZT  \n",
       "14               GESCHAETZT  \n",
       "15               GESCHAETZT  \n",
       "16               GESCHAETZT  \n",
       "17               GESCHAETZT  \n",
       "18               GESCHAETZT  \n",
       "19               GESCHAETZT  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips5.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functions.udf\n",
    "def create_interval(date):\n",
    "    # rush_hour = 6/9 17/19 = 0\n",
    "    # not_rush_hour = 0/6 9/17 19/24  = 1\n",
    "    date = date.split(' ')[1]\n",
    "    if (date >= '06:00:00' and date <= '09:00:00') or (date >= '17:00:00' and date <= '19:00:00'):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the arrival and departure delay of each stop station\n",
    "df3 = df2.withColumn(\"delay_arrival\", \n",
    "                     functions.unix_timestamp('real_arrival', 'yyyy-MM-dd HH:mm:ss') -\n",
    "                     functions.unix_timestamp('schedule_arrival', 'yyyy-MM-dd HH:mm:ss'))\n",
    "df3 = df3.withColumn(\"delay_dep\",\n",
    "                     functions.unix_timestamp('real_dep', 'yyyy-MM-dd HH:mm:ss') -\n",
    "                     functions.unix_timestamp('schedule_dep', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# We create time interval in order to differentiate rush hour and other time of the day\n",
    "# We have to try different combinations of interval time (that gave best prediction)\n",
    "df3 = df3.withColumn('arrival_interval', create_interval(df3.schedule_arrival))\n",
    "\n",
    "df3 = df3.withColumn('dep_interval', create_interval(df3.schedule_dep))\n",
    "\n",
    "# We can drop column that are not useful in order to make the delay predictions\n",
    "df3 = df3.drop('date', 'train_id', 'train_type', 'additional_trip', \n",
    "               'trip_failed', 'stop_name', 'stop_type', 'no_stop_here', \n",
    "               'schedule_time', 'real_dep', 'real_arrival', 'schedule_dep', 'schedule_arrival')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create 2 dataframes, one for the arrival (without the first departure stations)\n",
    "arrival_df = df3.filter(df3.arr_forecast_status == \"GESCHAETZT\")\n",
    "arrival_df = arrival_df.drop('dep_interval', 'delay_dep', 'arr_forecast_status', 'dep_forecast_status')\n",
    "\n",
    "# and another with the departure (without the last drop stations)\n",
    "dep_df = df3.filter(df3.dep_forecast_status == \"GESCHAETZT\")\n",
    "dep_df = dep_df.drop('arrival_interval', 'delay_arrival', 'arr_forecast_status', 'dep_forecast_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "\n",
    "trip_Indexer = StringIndexer(inputCol=\"trip_id\", outputCol=\"trip_id_index\")\n",
    "trip_model = trip_Indexer.fit(arrival_df)\n",
    "\n",
    "arrival_df = trip_model.transform(arrival_df)\n",
    "\n",
    "trans_Indexer = StringIndexer(inputCol=\"transport_type\", outputCol=\"transport_type_index\")\n",
    "trans_model = trans_Indexer.fit(arrival_df)\n",
    "\n",
    "arrival_df = trans_model.transform(arrival_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol=\"transport_type_index\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(arrival_df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_df = arrival_df.drop('trip_id', 'transport_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_df = arrival_df.withColumn('weekday', arrival_df['weekday'].cast('int'))\n",
    "arrival_df = arrival_df.withColumn('arrival_interval', arrival_df['arrival_interval'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_df = arrival_df.withColumn('delay_arrival', arrival_df['delay_arrival'] / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "vec_ass = VectorAssembler(inputCols=['stop_id','weekday','arrival_interval','trip_id_index','transport_type_index'], outputCol=\"features\")\n",
    "\n",
    "#arrival_df = vec_ass.transform(arrival_df)\n",
    "lr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", \n",
    "                      labelCol='delay_arrival')\n",
    "model = lr.fit(arrival_df)\n",
    "\n",
    "test = model.transform(arrival_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn('evaluation', test['delay_arrival'] - test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe('evaluation').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe('delay_arrival').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = arrival_df.limit(50).toPandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
