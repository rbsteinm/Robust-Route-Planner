{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "import math\n",
    "import helpers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '10g')\n",
    "conf.set('spark.executor.instances', '10')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.distance_to_zh.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/2017/10', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'driving_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows with stops near zurich\n",
    "df2 = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is still 51'571'541 rows in zurich area\n",
    "# df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select('stop_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functions.udf\n",
    "def date_choice(col1, col2):\n",
    "    return col1 if col2 is None else col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('schedule_time', date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(['driving_id', 'schedule_time', 'stop_id']).orderBy(['driving_id', 'schedule_time'], ascending=[0,0]).where(df2.driving_id == '85:773:18010-11456-1').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
