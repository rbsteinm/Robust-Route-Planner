{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers, spark_helpers\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import copy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "ZH_HB_ID = 8503000\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe. We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')\n",
    "\n",
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))\n",
    "\n",
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402\n",
    "\n",
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]\n",
    "\n",
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)\n",
    "# create dict of stations from pandas dataframe\n",
    "stations = pandas_df.set_index('station_ID').to_dict('index')\n",
    "\n",
    "# dump metadata in pickle\n",
    "with open('./data/metadata.pickle', 'wb') as handle:\n",
    "    pickle.dump(stations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load metadata from pickle\n",
    "with open('./data/metadata.pickle', 'rb') as handle:\n",
    "    stations = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load full data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', spark_helpers.get_weekday(df.date))\n",
    "\n",
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))\n",
    "\n",
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()\n",
    "\n",
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '2017-12-10 00:00:00')\n",
    "\n",
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')\n",
    "\n",
    "# discard ill-formated rows where the train leaves a station before arriving in it\n",
    "df2 = df2.where((df2.schedule_dep >= df2.schedule_arrival) | functions.col('schedule_arrival').isNull() | functions.col('schedule_dep').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', spark_helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', spark_helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep', 'line', 'transport_type', 'train_type', 'arr_forecast_status', 'weekday', 'real_arrival']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"arr_forecast_status\").over(w).alias(\"next_arr_forecast_status\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"real_arrival\").over(w).alias(\"next_real_arrival\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', spark_helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type, trips2.schedule_dep,trips2.next_sched_arr))\n",
    "\n",
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regenerate_models = False\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = (helpers.model_network(trips4, date))\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            helpers.network_to_datetime(network) # works inplace\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the networks from the pickles\n",
    "models = []\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)\n",
    "    print(day + ' loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute walking network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute walking network\n",
    "walking_network = helpers.compute_walking_network(stations)\n",
    "print('walking network loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model :\n",
    "\n",
    "In this part we are going to build a preditive model using historical arrival/departure time data. Thus, our goal is to predict an uncertainty or certainty rate of taking a transport change with success, and finally to spread it for a whole journey using a route planning algorithm explained in the next section.\n",
    "\n",
    "### 1) Processing the datas :\n",
    "\n",
    "First of all, we have to group the data that match (same trip between two transport stops at the same schedule), and to compute the delay for the trips where the historical time was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop what we do not need for the prediction\n",
    "trips5 = trips4.drop('stop_type', 'next_type', 'is_valid', 'arr_forcast_status', \n",
    "                     'schedule_time', 'schedule_arrival', 'next_sched_dep', \n",
    "                     'next_time', 'next_tid', 'real_arrival', 'arr_forecast_status')\n",
    "\n",
    "trips5 = trips5.na.fill(\"temp 00:00:00\")\n",
    "\n",
    "# We refactor the dates by keeping only the time of the day and not the entire day\n",
    "trips5 = trips5.withColumn('schedule_dep', spark_helpers.keep_time(trips5.schedule_dep))\n",
    "trips5 = trips5.withColumn('next_sched_arr', spark_helpers.keep_time(trips5.next_sched_arr))\n",
    "trips5 = trips5.withColumn('next_real_arrival', spark_helpers.keep_time(trips5.next_real_arrival))\n",
    "\n",
    "# We compute the arrival delay of each stop station\n",
    "df3 = trips5.withColumn(\"delay_arrival\", \n",
    "                     functions.unix_timestamp('next_real_arrival', 'HH:mm:ss') -\n",
    "                     functions.unix_timestamp('next_sched_arr', 'HH:mm:ss'))\n",
    "\n",
    "# We create time interval in order to differentiate rush hour and other time of the day\n",
    "# We have to try different combinations of interval time (that gave best prediction)\n",
    "df3 = df3.withColumn('arrival_interval', spark_helpers.create_rush(df3.next_sched_arr))\n",
    "# We also gather the week day in 4 buckets (Wednesnay, Saturday and Sunday separatly and the other days together)\n",
    "df3 = df3.withColumn('weekday', spark_helpers.group_weekday(df3.next_sched_arr))\n",
    "\n",
    "# We create a dataframe where we only keep the rows where there is a real arrival time\n",
    "arrival_df1 = df3.filter(df3.next_arr_forecast_status == \"GESCHAETZT\")\n",
    "arrival_df1 = arrival_df1.drop('next_arr_forecast_status')\n",
    "\n",
    "# arrival_df.groupby('transport_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see only the trains are concerned with historical time data, thus we were able to compute the delay only for those trips. Moreover we only had 1 millions rows of evaluated time on the 30 millions trips that we have. \n",
    "\n",
    "So, one solution was to consider that there is no delay for the other trips (Bus / Tram), thus, there will almost alway have 0% chance to miss a connection between two transports as there are a majority of Bus and Tram in the center of Zurich.\n",
    "\n",
    "An other solution was to try to predict the delay of those trips according to the ones we already had. That is why we decided to make a linear regression using the localization (latitude, longitude) of the stop, as along with the day and time of the trip.\n",
    "\n",
    "### 2) Complete the delay data with Linear Regression :\n",
    "\n",
    "Here we tried to predict the delay for every station. We made the assumption to use the delay of the train to predict the delay of the bus and trams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dataframe of stations with unknown delays\n",
    "#station_to_predict = df3.filter(df3.next_arr_forecast_status == \"PROGNOSE\").groupby(['transport_type', 'next_sid', 'arrival_interval', 'weekday']).count()\n",
    "\n",
    "#station_to_predict = station_to_predict.toPandas()\n",
    "\n",
    "regenerate_knn_pickle = False\n",
    "if regenerate_knn_pickle:\n",
    "    station_to_predict.to_pickle('./data/delay_knn.pickle')\n",
    "else:\n",
    "    station_to_predict = pd.read_pickle('./data/delay_knn.pickle')\n",
    "\n",
    "#Add longitutude and latitude\n",
    "station_to_predict['long'] = station_to_predict['next_sid'].map(lambda x: pandas_df[pandas_df['station_ID'] == x].long.values[0])\n",
    "station_to_predict['lat'] = station_to_predict['next_sid'].map(lambda x: pandas_df[pandas_df['station_ID'] == x].lat.values[0])\n",
    "\n",
    "#Dataframe of stations with known delays\n",
    "station_computed = arrival_df1.withColumn('long', spark_helpers.get_longitude(arrival_df1.next_sid))\n",
    "station_computed = station_computed.withColumn('lat', spark_helpers.get_latitude(station_computed.next_sid))\n",
    "\n",
    "#station_computed_pandas = station_computed.toPandas()\n",
    "regenerate_stat_comp_pickle = False\n",
    "if regenerate_stat_comp_pickle:\n",
    "    station_computed_pandas.to_pickle('./data/stat_comp.pickle')\n",
    "else:\n",
    "    station_computed_pandas = pd.read_pickle('./data/stat_comp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_computed_lin = station_computed_pandas[['trip_id', 'next_sid', 'weekday', 'arrival_interval', 'long', 'lat', 'delay_arrival']]\n",
    "\n",
    "#We use a linear regression to predict the delay using the localisation, the weekday and  the time interval\n",
    "lr = LinearRegression()\n",
    "lr.fit(station_computed_lin[['weekday', 'arrival_interval', 'long', 'lat']], station_computed_lin['delay_arrival'])\n",
    "\n",
    "station_to_predict['pred'] = lr.predict(station_to_predict[['weekday', 'arrival_interval', 'long', 'lat']])\n",
    "\n",
    "\n",
    "def get_delay(status, delay, interval, week_day, transport_type, sid):\n",
    "    if status == 'GESCHAETZT':\n",
    "        return float(delay)\n",
    "    return float(station_to_predict[(station_to_predict['next_sid'] == sid) & \n",
    "                              (station_to_predict['transport_type'] == transport_type) & \n",
    "                              (station_to_predict['weekday'] == week_day) & \n",
    "                              (station_to_predict['arrival_interval'] == interval)]['pred'])\n",
    "\n",
    "get_delay_udf = functions.udf(get_delay, FloatType())                                                                                                                                                    \n",
    "\n",
    "#Set the delay to every stations. Keep the orignal if exist otherwise use the prediction\n",
    "arrival_df = df3.withColumn('real_delay', get_delay_udf(df3.next_arr_forecast_status, df3.delay_arrival, df3.arrival_interval, df3.weekday, df3.transport_type, df3.next_sid))\n",
    "\n",
    "arrival_df = arrival_df.drop('delay_arrival')\n",
    "arrival_df = arrival_df.withColumnRenamed(\"real_delay\", \"delay_arrival\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Evaluate the uncertainty according to the delay:\n",
    "\n",
    "Thus for now we have the delay for a majority of trips for a weekday and a time interval. Thus we have to find a way to evaluate the uncertainty according to the delays we have. We decide to use the interquartile range (IQR) in order to manage the outliers. In fact, the interquartile range is the difference between the upper and lower quartiles (75th, 25th percentiles) Q3 and Q2. \n",
    "\n",
    "$IQR = Q3 - Q2$\n",
    "\n",
    "It allows us to mesure the dispersion of the delays for each trip. Moreover we associate it with the interquartile mean (IQM) which is the mean of the data included in the interquartile range and is insensitive to outliers.\n",
    "\n",
    "$IQM = \\frac{2}{n}\\sum_{i = \\frac{n}{4}+1}^\\frac{3n}{4} x_i$\n",
    "\n",
    "Thus for each trips, for a given day of the week and time interval, we now have an estimation of the worst delay possible without being affected by the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We group the trips by their week day and time interval\n",
    "delay_distribution = arrival_df.groupby(['trip_id', 'arrival_interval', 'weekday']).agg(functions.collect_list(functions.col('delay_arrival'))).alias('distri')\n",
    "delay_distribution = delay_distribution.withColumn('distri', spark_helpers.delete_neg(functions.col('collect_list(delay_arrival)')))\n",
    "\n",
    "# We compute the IQM / IQR and worst case\n",
    "delay_distribution = delay_distribution.withColumn('IQM', spark_helpers.iqm(delay_distribution.distri))\n",
    "delay_distribution = delay_distribution.withColumn('interquartile', spark_helpers.interquartile(delay_distribution.distri))\n",
    "delay_distribution = delay_distribution.withColumn('worst_case', delay_distribution.IQM + delay_distribution.interquartile)\n",
    "\n",
    "#delay_distribution_pd = delay_distribution.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then we store it in order to easily make our predictions.\n",
    "regenerate_distri_pickle = False\n",
    "if regenerate_distri_pickle:\n",
    "    delay_distribution_pd.to_pickle('./data/delay_distri.pickle')\n",
    "else:\n",
    "    #delay_distribution_pd = pd.read_pickle('./data/delay_distri.pickle')\n",
    "    # if we want to use the predictions\n",
    "    delay_distribution_pd = pd.read_pickle('./data/full_delay_distri.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example\n",
    "sp = helpers.shortest_path(models, walking_network, stations,ZH_HB_ID, 8502559, datetime(2018, 1, 15, 14))\n",
    "helpers.reduced_path_tostring(helpers.reduce_path(sp), stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out the stations that are not reachable from Zürich HB on mondays\n",
    "reachable_stations_ids = helpers.get_reachable_stations(models[0], walking_network, ZH_HB_ID)\n",
    "reachable_stations = {sid: stations[sid] for sid in reachable_stations_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[6][ZH_HB_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select two stops at random and create a shortest path. Do that 100 times\n",
    "shortest_paths = []\n",
    "n_runs = 100\n",
    "date = datetime(2018, 3, 15, 14)\n",
    "for i in range(n_runs):\n",
    "    if i%10==0:\n",
    "        print(100*i/n_runs, '% finished')\n",
    "    #source = random.choice(list(stations.keys()))\n",
    "    dest = random.choice(list(reachable_stations.keys()))\n",
    "    shortest_paths.append(helpers.shortest_path(models, walking_network, reachable_stations, ZH_HB_ID, dest, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path with arrival time\n",
    "To compute the shortest path with arrival time instead of departure time, we implemented a reversed version of our shortest path algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = helpers.shortest_path_reverse(models, walking_network, reachable_stations, ZH_HB_ID, 8590727, datetime(2018, 1, 15, 23,5))\n",
    "helpers.reduced_path_tostring(helpers.reduce_path(sp), stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining uncertainty of a trip according to the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(shortest_paths):\n",
    "    print(i, helpers.routing_algo(p, delay_distribution_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the total trip for the given shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = helpers.safest_paths(models, walking_network, reachable_stations, ZH_HB_ID, 8587978, datetime(2018, 3, 15, 14), delay_distribution_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(helpers.compute_path_time(x[0])) for x in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_trip(pandas_df, res[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_trip(pandas_df, res[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- iterative example:\n",
    "    - first show a shortest path with no probability\n",
    "    - then show that there is a high proba to miss it\n",
    "    - then explain how we will solve it\n",
    "    - 1. show and explain the routing algo\n",
    "    - 2. explain how we integrated the routing algo to generate new safest paths\n",
    "    - 3. explain the viz with bokeh to ensure that the paths look normal\n",
    "        - show the isochrome map\n",
    "- integrate the different paths in the front-end\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
