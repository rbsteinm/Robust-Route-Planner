{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab in Data Science: Final Project\n",
    "\n",
    "Pierre Fouche, Matthias Leroy and Raphaël Steinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as functions\n",
    "from pyspark.sql.types import BooleanType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "import helpers, spark_helpers\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import copy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "ZH_HB_ID = 8503000\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata\n",
    "First, let's clean the metadata dataframe. We will use the SBB data limited around the Zurich area. We will focus on all the stops within 10km of the Zurich train station. Let's get rid of all the stations that are too far away from Zurich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "raw_metadata = spark.read.load('/datasets/project/metadata', format='com.databricks.spark.csv', header='false', sep='\\\\t')\n",
    "\n",
    "# remove multiple spaces\n",
    "metadata = raw_metadata.withColumn('_c0', functions.regexp_replace(raw_metadata._c0, '\\s+', ' '))\n",
    "# split into columns\n",
    "metadata = metadata.withColumn('name', functions.split(metadata._c0, '%')[1])\n",
    "for (name, index, type_) in [('station_ID',0, 'int'), ('long',1, 'double'), ('lat',2, 'double'), ('height',3, 'int')]:\n",
    "    metadata = metadata.withColumn(name, functions.split(metadata._c0, ' ')[index].cast(type_))\n",
    "# remove useless column\n",
    "metadata = metadata.drop('_c0')\n",
    "# trim name column to remove left/right blank\n",
    "metadata = metadata.withColumn('name', functions.trim(metadata.name))\n",
    "\n",
    "# coordinates of Zürich main train station\n",
    "lat_zurich = 47.3782\n",
    "long_zurich = 8.5402\n",
    "\n",
    "# convert to pandas dataframe\n",
    "pandas_df = metadata.toPandas()\n",
    "\n",
    "# keep only the stops that are located < 10km from Zurich HB\n",
    "pandas_df['distance_to_zh'] = pandas_df.apply(lambda x: helpers.distance(x['long'], x['lat'], long_zurich, lat_zurich), axis=1)\n",
    "pandas_df = pandas_df[pandas_df['distance_to_zh'] < 10]\n",
    "\n",
    "# recreate spark dataframe from pandas dataframe\n",
    "metadata = spark.createDataFrame(pandas_df)\n",
    "# create dict of stations from pandas dataframe\n",
    "stations = pandas_df.set_index('station_ID').to_dict('index')\n",
    "\n",
    "# dump metadata in pickle\n",
    "with open('./data/metadata.pickle', 'wb') as handle:\n",
    "    pickle.dump(stations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata from pickle\n",
    "with open('./data/metadata.pickle', 'rb') as handle:\n",
    "    stations = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data\n",
    "raw_df = spark.read.load('/datasets/project/istdaten/*/*', format='csv', header='true', inferSchema='true', sep=';')\n",
    "# load sample data\n",
    "# raw_df = spark.read.load('/datasets/project/istdaten/2018/01', format='csv', header='true', inferSchema='true', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "\n",
    "df = raw_df.selectExpr([k + ' as ' + fields[k] for k in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor dates\n",
    "df = df.withColumn('date', functions.from_unixtime(functions.unix_timestamp('date', 'dd.MM.yyyy')))\n",
    "df = df.withColumn('schedule_arrival', functions.from_unixtime(functions.unix_timestamp('schedule_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_arrival', functions.from_unixtime(functions.unix_timestamp('real_arrival', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('schedule_dep', functions.from_unixtime(functions.unix_timestamp('schedule_dep', 'dd.MM.yyyy HH:mm')))\n",
    "df = df.withColumn('real_dep', functions.from_unixtime(functions.unix_timestamp('real_dep', 'dd.MM.yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column containing the weekday (monday=1, sunday=6)\n",
    "df = df.withColumn('weekday', spark_helpers.get_weekday(df.date))\n",
    "\n",
    "# keep only the rows with stops near zurich\n",
    "df = df.where(df.stop_id.isin([int(x) for x in list(pandas_df.station_ID.unique())]))\n",
    "\n",
    "# there is still 51'571'541 rows in zurich area\n",
    "# df.count()\n",
    "\n",
    "# keep only date after the 10th of december, because the schedule changed\n",
    "df = df.where(df.date > '2017-12-10 00:00:00')\n",
    "\n",
    "# discard the rows when there is no stop here\n",
    "df2 = df.where(df.no_stop_here == 'false')\n",
    "\n",
    "# discard ill-formated rows where the train leaves a station before arriving in it\n",
    "df2 = df2.where((df2.schedule_dep >= df2.schedule_arrival) | functions.col('schedule_arrival').isNull() | functions.col('schedule_dep').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the network\n",
    "\n",
    "### From stops to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column with the schedule time that will be used to build the network\n",
    "df2 = df2.withColumn('schedule_time', spark_helpers.date_choice(df2.schedule_arrival, df2.schedule_dep))\n",
    "#df2 = df2.withColumn('schedule_time', functions.from_unixtime(functions.unix_timestamp('schedule_time', 'dd.MM.yyyy HH:mm')))\n",
    "\n",
    "# create a column that tells if a stop is the first/last one of its trip or in the middle\n",
    "df2 = df2.withColumn('stop_type', spark_helpers.stop_type(df2.schedule_dep, df2.schedule_arrival))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = df2.select(['trip_id', 'date', 'schedule_time', 'stop_id', 'stop_type', 'schedule_arrival', 'schedule_dep', 'line', 'transport_type', 'train_type', 'arr_forecast_status', 'weekday', 'real_arrival']).orderBy(['trip_id', 'schedule_time'], ascending=[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# duplicate the dataframe, shift the copy of one row and append it to the original\n",
    "# this way, we have for each row the current stop and the next stop\n",
    "w = Window().partitionBy(functions.col('trip_id')).orderBy(functions.col('trip_id'))\n",
    "trips2 = trips.select(\"*\", functions.lag(\"trip_id\").over(w).alias(\"next_tid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_time\").over(w).alias(\"next_time\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_id\").over(w).alias(\"next_sid\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"stop_type\").over(w).alias(\"next_type\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_arrival\").over(w).alias(\"next_sched_arr\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"schedule_dep\").over(w).alias(\"next_sched_dep\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"arr_forecast_status\").over(w).alias(\"next_arr_forecast_status\"))\n",
    "trips2 = trips2.select(\"*\", functions.lag(\"real_arrival\").over(w).alias(\"next_real_arrival\"))\n",
    "\n",
    "trips2 = trips2.where(trips2.next_time.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column telling if the edge is valid or not\n",
    "# (i.e. if the stop and next stop are really part of the same ride)\n",
    "trips3 = trips2.withColumn('is_valid', spark_helpers.edge_is_valid(trips2.trip_id, trips2.schedule_time, trips2.stop_id, trips2.stop_type, trips2.next_tid, trips2.next_time, trips2.next_sid, trips2.next_type, trips2.schedule_dep,trips2.next_sched_arr))\n",
    "\n",
    "# keep only valid edges\n",
    "trips4 = trips3.filter(trips3.is_valid=='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each day of the week, model the network\n",
    "Get the edges of the network and the departure/arrival times for each trip (edge=trip)\n",
    "We assume the schedule repeat every week, and we generate one schedule per weekday.\n",
    "Days off have the same schedules as sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model for each day of the week\n",
    "# this code needs to be run only once\n",
    "typical_monday = '2018-01-15 00:00:00'\n",
    "typical_tuesday = '2018-01-16 00:00:00'\n",
    "typical_wednesday = '2018-01-17 00:00:00'\n",
    "typical_thursday = '2018-01-18 00:00:00'\n",
    "typical_friday = '2018-01-19 00:00:00'\n",
    "typical_saturday = '2018-01-20 00:00:00'\n",
    "typical_sunday = '2018-01-21 00:00:00'\n",
    "typical_week = [typical_monday,typical_tuesday,typical_wednesday,typical_thursday,typical_friday,typical_saturday,typical_sunday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regenerate_models = False\n",
    "days_names = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "\n",
    "# generate one network for each weekday and store them in pickles\n",
    "if regenerate_models:\n",
    "    for (date, day_name) in zip(typical_week, days_names):\n",
    "        network = (helpers.model_network(trips4, date))\n",
    "        with open('./data/'+day_name+'.pickle', 'wb') as handle:\n",
    "            helpers.network_to_datetime(network) # works inplace\n",
    "            pickle.dump(network, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(str(day_name) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the networks from the pickles\n",
    "models = []\n",
    "for day in days_names:\n",
    "    with open('./data/'+ day +'.pickle', 'rb') as handle:\n",
    "        network = pickle.load(handle)\n",
    "    models.append(network)\n",
    "    print(day + ' loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute walking network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute walking network\n",
    "walking_network = helpers.compute_walking_network(stations)\n",
    "print('walking network loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model :\n",
    "\n",
    "In this part we are going to build a preditive model using historical arrival/departure time data. Thus, our goal is to predict an uncertainty or certainty rate of taking a transport change with success, and finally to spread it for a whole journey using a route planning algorithm explained in the next section.\n",
    "\n",
    "### 1) Processing the datas :\n",
    "\n",
    "First of all, we have to group the data that match (same trip between two transport stations at the same schedule), and to compute the delay for the trips where the historical time was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop what we do not need for the prediction\n",
    "trips5 = trips4.drop('stop_type', 'next_type', 'is_valid', 'arr_forcast_status', \n",
    "                     'schedule_time', 'schedule_arrival', 'next_sched_dep', \n",
    "                     'next_time', 'next_tid', 'real_arrival', 'arr_forecast_status')\n",
    "\n",
    "trips5 = trips5.na.fill(\"temp 00:00:00\")\n",
    "\n",
    "# We refactor the dates by keeping only the time of the day and not the entire day\n",
    "trips5 = trips5.withColumn('schedule_dep', spark_helpers.keep_time(trips5.schedule_dep))\n",
    "trips5 = trips5.withColumn('next_sched_arr', spark_helpers.keep_time(trips5.next_sched_arr))\n",
    "trips5 = trips5.withColumn('next_real_arrival', spark_helpers.keep_time(trips5.next_real_arrival))\n",
    "\n",
    "# We compute the arrival delay of each stop station\n",
    "df3 = trips5.withColumn(\"delay_arrival\", \n",
    "                     functions.unix_timestamp('next_real_arrival', 'HH:mm:ss') -\n",
    "                     functions.unix_timestamp('next_sched_arr', 'HH:mm:ss'))\n",
    "\n",
    "create_rush = functions.udf(helpers.rush_inter)\n",
    "group_weekday = functions.udf(helpers.group_weekday_py)\n",
    "\n",
    "# We create time interval in order to differentiate rush hour and other time of the day\n",
    "# We have to try different combinations of interval time (that gave best prediction)\n",
    "df3 = df3.withColumn('arrival_interval', create_rush(df3.next_sched_arr))\n",
    "# We also gather the week day in 4 buckets (Wednesnay, Saturday and Sunday separatly and the other days together)\n",
    "df3 = df3.withColumn('weekday', group_weekday(df3.next_sched_arr))\n",
    "\n",
    "# We create a dataframe where we only keep the rows where there is a real arrival time\n",
    "arrival_df1 = df3.filter(df3.next_arr_forecast_status == \"GESCHAETZT\")\n",
    "arrival_df1 = arrival_df1.drop('next_arr_forecast_status')\n",
    "\n",
    "# arrival_df.groupby('transport_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed that only the trains are concerned with historical time data, thus we were able to compute the delay only for those trips. Moreover we only had 1 millions rows of evaluated time on the 30 millions trips that we have. \n",
    "\n",
    "So, one solution was to consider that there is no delay for the other trips (Bus / Tram), thus, there will almost alway have 0% chance to miss a connection between two transports as there are a majority of Bus and Tram in the center of Zurich.\n",
    "\n",
    "An other solution was to try to predict the delay of those trips according to the ones we already had. That is why we decided to make a linear regression using the localization (latitude, longitude) of the stop, as along with the day and time of the trip.\n",
    "\n",
    "### 2) Complete the delay data with Linear Regression :\n",
    "\n",
    "Here we tried to predict the delay for every station. We made the assumption to use the delay of the train to predict the delay of the bus and trams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dataframe of stations with unknown delays\n",
    "#station_to_predict = df3.filter(df3.next_arr_forecast_status == \"PROGNOSE\").groupby(['transport_type', 'next_sid', 'arrival_interval', 'weekday']).count()\n",
    "\n",
    "#station_to_predict = station_to_predict.toPandas()\n",
    "\n",
    "regenerate_knn_pickle = False\n",
    "if regenerate_knn_pickle:\n",
    "    station_to_predict.to_pickle('./data/delay_knn.pickle')\n",
    "else:\n",
    "    station_to_predict = pd.read_pickle('./data/delay_knn.pickle')\n",
    "\n",
    "#Add longitutude and latitude\n",
    "station_to_predict['long'] = station_to_predict['next_sid'].map(lambda x: pandas_df[pandas_df['station_ID'] == x].long.values[0])\n",
    "station_to_predict['lat'] = station_to_predict['next_sid'].map(lambda x: pandas_df[pandas_df['station_ID'] == x].lat.values[0])\n",
    "\n",
    "#Dataframe of stations with known delays\n",
    "station_computed = arrival_df1.withColumn('long', spark_helpers.get_longitude(arrival_df1.next_sid))\n",
    "station_computed = station_computed.withColumn('lat', spark_helpers.get_latitude(station_computed.next_sid))\n",
    "\n",
    "#station_computed_pandas = station_computed.toPandas()\n",
    "regenerate_stat_comp_pickle = False\n",
    "if regenerate_stat_comp_pickle:\n",
    "    station_computed_pandas.to_pickle('./data/stat_comp.pickle')\n",
    "else:\n",
    "    station_computed_pandas = pd.read_pickle('./data/stat_comp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_computed_lin = station_computed_pandas[['trip_id', 'next_sid', 'weekday', 'arrival_interval', 'long', 'lat', 'delay_arrival']]\n",
    "\n",
    "#We use a linear regression to predict the delay using the localisation, the weekday and  the time interval\n",
    "lr = LinearRegression()\n",
    "lr.fit(station_computed_lin[['weekday', 'arrival_interval', 'long', 'lat']], station_computed_lin['delay_arrival'])\n",
    "\n",
    "station_to_predict['pred'] = lr.predict(station_to_predict[['weekday', 'arrival_interval', 'long', 'lat']])\n",
    "\n",
    "\n",
    "def get_delay(status, delay, interval, week_day, transport_type, sid):\n",
    "    if status == 'GESCHAETZT':\n",
    "        return float(delay)\n",
    "    return float(station_to_predict[(station_to_predict['next_sid'] == sid) & \n",
    "                              (station_to_predict['transport_type'] == transport_type) & \n",
    "                              (station_to_predict['weekday'] == week_day) & \n",
    "                              (station_to_predict['arrival_interval'] == interval)]['pred'])\n",
    "\n",
    "get_delay_udf = functions.udf(get_delay, FloatType())                                                                                                                                                    \n",
    "\n",
    "#Set the delay to every stations. Keep the orignal if exist otherwise use the prediction\n",
    "arrival_df = df3.withColumn('real_delay', get_delay_udf(df3.next_arr_forecast_status, df3.delay_arrival, df3.arrival_interval, df3.weekday, df3.transport_type, df3.next_sid))\n",
    "\n",
    "arrival_df = arrival_df.drop('delay_arrival')\n",
    "arrival_df = arrival_df.withColumnRenamed(\"real_delay\", \"delay_arrival\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Evaluate the uncertainty according to the delay:\n",
    "\n",
    "Thus for now we have the delay for a majority of trips for a weekday and a time interval. Thus we have to find a way to evaluate the uncertainty according to the delays we have. We decide to use the interquartile range (IQR) in order to manage the outliers. In fact, the interquartile range is the difference between the upper and lower quartiles (75th, 25th percentiles) Q3 and Q2. \n",
    "\n",
    "$IQR = Q3 - Q2$\n",
    "\n",
    "It allows us to mesure the dispersion of the delays for each trip. Moreover we associate it with the interquartile mean (IQM) which is the mean of the data included in the interquartile range and is insensitive to outliers.\n",
    "\n",
    "$IQM = \\frac{2}{n}\\sum_{i = \\frac{n}{4}+1}^\\frac{3n}{4} x_i$\n",
    "\n",
    "Thus for each trips, for a given day of the week and time interval, we now have an estimation of the **worst delay** possible without being affected by the outliers.\n",
    "\n",
    "We have decided to group the Monday, Tuesday, Thursday and Friday as we considered them as typical working days, the 3 other days were processed separatly. Morevover in order to create the time intervals we have decided to group the \"rush hours\" together (6h/9h - 17h/19h) separatly to the others time of the day. First of all, we made a Kmeans based on the delay, the week days and the transport types in order to determine those intervals however it does not give us specific time clusters that is why we used those ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We group the trips by their week day and time interval\n",
    "delay_distribution = arrival_df.groupby(['trip_id', 'arrival_interval', 'weekday']).agg(functions.collect_list(functions.col('delay_arrival'))).alias('distri')\n",
    "delay_distribution = delay_distribution.withColumn('distri', spark_helpers.delete_neg(functions.col('collect_list(delay_arrival)')))\n",
    "\n",
    "# We compute the IQM / IQR and worst case\n",
    "delay_distribution = delay_distribution.withColumn('IQM', spark_helpers.iqm(delay_distribution.distri))\n",
    "delay_distribution = delay_distribution.withColumn('interquartile', spark_helpers.interquartile(delay_distribution.distri))\n",
    "delay_distribution = delay_distribution.withColumn('worst_case', delay_distribution.IQM + delay_distribution.interquartile)\n",
    "\n",
    "#delay_distribution_pd = delay_distribution.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then we store it in order to easily make our predictions.\n",
    "regenerate_distri_pickle = False\n",
    "if regenerate_distri_pickle:\n",
    "    delay_distribution_pd.to_pickle('./data/delay_distri.pickle')\n",
    "else:\n",
    "    #delay_distribution_pd = pd.read_pickle('./data/delay_distri.pickle')\n",
    "    # if we want to use the predictions\n",
    "    delay_distribution_pd = pd.read_pickle('./data/full_delay_distri.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see an example of delay worst cases in seconds for some trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_distribution_pd[[\"trip_id\", \"IQM\", \"interquartile\", \"worst_case\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- iterative example:\n",
    "    - first show a shortest path with no probability\n",
    "    - then show that there is a high proba to miss it\n",
    "    - then explain how we will solve it\n",
    "    - 1. show and explain the routing algo\n",
    "    - 2. explain how we integrated the routing algo to generate new safest paths\n",
    "    - 3. explain the viz with bokeh to ensure that the paths look normal\n",
    "        - show the isochrome map\n",
    "- integrate the different paths in the front-end\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path algorithm\n",
    "\n",
    "# RAAAAAAAAAAAPH Parler du fait qu'on peut donner heure de depart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example\n",
    "sp = helpers.shortest_path(models, walking_network, stations,ZH_HB_ID, 8591085, datetime(2018, 1, 15, 14))\n",
    "helpers.reduced_path_tostring(helpers.reduce_path(sp), stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see a typical trip from the Zurich main train station to the stop Zürich, Birch-/Glatttalstrasse.\n",
    "However we can ask ourself what is the reliability of this journey. Do we have a big chance of missing a change ?\n",
    "\n",
    "## Reliability of each connection and the whole trip\n",
    "\n",
    "Thus we can compute the feasibility of the trip. We estimat the reliability of each change by making the ratio between the scheduled period of time we have to take the connection with the worst case delay that can happened to the previous transport. This worst case delay has been computed in the previous part for all trips. \n",
    "\n",
    "Finally, we determine the global uncertainty score by multiplying the score of each change of the trip. This score is spread from 0 (certain to miss the change) to 1 (certain to take the change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = helpers.routing_algo(sp, delay_distribution_pd)\n",
    "for i,s in enumerate(score[0].values()):\n",
    "    print(\" Change number {} : {}% of chance to take it with success\".format(i+1, round(s*100)))\n",
    "\n",
    "print(\"\\n This trip is realisable {}% of the time \".format(round(score[1]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the route from Zürich HB to Zürich, Birch-/Glatttalstrasse is the fastest at least 35% of the time if I want to leave at 14h.\n",
    "\n",
    "And it is obviously the first change, that has a problem. The line IR70 has a too big chance to be late, and thus the passenger will not succed to take the line 768 **35%** of the time after joining the station by walking.\n",
    "\n",
    "Finally, we have to let the posibility to choose a safest trip even if it is a bit slower, in order to reassure traveler that wants to arrive at all cost.\n",
    "\n",
    "## Safest trip\n",
    "\n",
    "In order to do so, we go to our network graph and delete the edge with the smallest certainty score. Then we rerun the shortest path algorithm until it finds a trip with a total score bigger than the threshold (than we fix here at 80%), we eventually stop after 100 runs if it does not find a 80% certain path.\n",
    "\n",
    "Then we show the fastest without regard of the feasibility of the trip and the 3 safest ones (if we do not find the best one before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the stations that are not reachable from Zürich HB on mondays\n",
    "reachable_stations_ids = helpers.get_reachable_stations(models[0], walking_network, ZH_HB_ID)\n",
    "reachable_stations = {sid: stations[sid] for sid in reachable_stations_ids}\n",
    "\n",
    "res = helpers.safest_paths(models, walking_network, reachable_stations, ZH_HB_ID, 8591085, datetime(2018, 3, 15, 14), delay_distribution_pd, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(res[:2]):    \n",
    "    print(\"Trip number {} as {}% of success and lasts {} \".format(i+1, round(t[1][1] * 100), str(helpers.compute_path_time(t[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the safest trip is not the fastest one but it has better chance to succed, thus it could be select by the users if he wants to be sur to arrive in time.\n",
    "\n",
    "## Visulatization\n",
    "\n",
    "We have create a Bokeh visualization in order to display those two trips and see if they make sense. It shows the different transport the user has to take and the station where there is a connection.\n",
    "\n",
    "We start with the fastest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_trip(pandas_df, res[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the safest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_trip(pandas_df, res[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOCHRONOUS MAP\n",
    "\n",
    "We can now print an isochrnous map, thus we can watch the maximal distance we can reach during a given time (in minutes) that we can change from the Zurich train station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_path(regenerate):\n",
    "    \"\"\"\n",
    "    Generates all shortest path for all the reachable stations\n",
    "    \"\"\"\n",
    "    all_paths = []\n",
    "    date = datetime(2018, 1, 15, 14)\n",
    "    if regenerate:\n",
    "        \n",
    "        for destination in list(reachable_stations.keys()):\n",
    "            if destination != ZH_HB_ID:\n",
    "                all_paths.append(helpers.shortest_path(models, walking_network, reachable_stations, ZH_HB_ID, destination, date))\n",
    "            \n",
    "        pickle.dump(all_paths, open('./data/all_paths.pickle', 'wb'))\n",
    "        \n",
    "    else:\n",
    "        all_paths = pickle.load(open('./data/all_paths.pickle', 'rb'))        \n",
    "    return all_paths\n",
    "\n",
    "# We can draw the isochrnous map for 15 minutes\n",
    "helpers.isoch(15, compute_all_path(False), pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPRIMER TOUT DESSOUS ???\n",
    "\n",
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the stations that are not reachable from Zürich HB on mondays\n",
    "reachable_stations_ids = helpers.get_reachable_stations(models[0], walking_network, ZH_HB_ID)\n",
    "reachable_stations = {sid: stations[sid] for sid in reachable_stations_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[6][ZH_HB_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select two stops at random and create a shortest path. Do that 100 times\n",
    "shortest_paths = []\n",
    "n_runs = 100\n",
    "date = datetime(2018, 3, 15, 14)\n",
    "for i in range(n_runs):\n",
    "    if i%10==0:\n",
    "        print(100*i/n_runs, '% finished')\n",
    "    #source = random.choice(list(stations.keys()))\n",
    "    dest = random.choice(list(reachable_stations.keys()))\n",
    "    shortest_paths.append(helpers.shortest_path(models, walking_network, reachable_stations, ZH_HB_ID, dest, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path with arrival time\n",
    "To compute the shortest path with arrival time instead of departure time, we implemented a reversed version of our shortest path algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = helpers.shortest_path_reverse(models, walking_network, reachable_stations, ZH_HB_ID, 8590727, datetime(2018, 1, 15, 23,5))\n",
    "helpers.reduced_path_tostring(helpers.reduce_path(sp), stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining uncertainty of a trip according to the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(shortest_paths):\n",
    "    print(i, helpers.routing_algo(p, delay_distribution_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_paths[76]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the total trip for the given shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = helpers.safest_paths(models, walking_network, reachable_stations, ZH_HB_ID, 8587978, datetime(2018, 3, 15, 14), delay_distribution_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(helpers.compute_path_time(x[0])) for x in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
